{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IXm5ZjlFl53"
      },
      "source": [
        "# Groq Tutorial - Ultra-Fast Inference\n",
        "\n",
        "This notebook covers working with Groq's lightning-fast LLM inference using the `llm_playbook` package.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Setting up the Groq client\n",
        "- Available models (Llama, Mixtral, Gemma)\n",
        "- Basic chat with incredibly fast responses\n",
        "- Streaming for real-time output\n",
        "\n",
        "## Available Models\n",
        "\n",
        "| Model | Description |\n",
        "|-------|-------------|\n",
        "| `llama-3.3-70b-versatile` | Latest Llama 3.3 (default) |\n",
        "| `llama-3.1-70b-versatile` | Llama 3.1 70B |\n",
        "| `llama-3.1-8b-instant` | Fast small model |\n",
        "| `mixtral-8x7b-32768` | Mixtral MoE |\n",
        "| `gemma2-9b-it` | Google Gemma 2 |\n",
        "\n",
        "## Why Groq?\n",
        "\n",
        "- **Blazing fast**: 10x faster than other providers\n",
        "- **LPU hardware**: Custom chips designed for LLM inference\n",
        "- **Free tier**: Generous limits for experimentation\n",
        "- **Open models**: Access to Llama, Mixtral, and more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7pUTT_WFl54"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install the package and configure your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Az1GzV7Fl54",
        "outputId": "d20c1764-d4e9-4db3-96a7-ab43bcf826dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install the package\n",
        "!pip install -q git+https://github.com/deepakdeo/python-llm-playbook.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EdFl5xHFl55",
        "outputId": "2d284756-142b-4b77-b95b-c55d97dd5de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key configured!\n"
          ]
        }
      ],
      "source": [
        "# Setup API Key from Colab Secrets\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Add your GROQ_API_KEY in the Secrets pane (ðŸ”‘ icon in left sidebar)\n",
        "# Get your key at: https://console.groq.com\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "print(\"API key configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbqIzF5DFl56"
      },
      "source": [
        "## 1. Basic Chat - Feel the Speed!\n",
        "\n",
        "Groq's main selling point is speed. Let's see it in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97Al43V-Fl56",
        "outputId": "32882ff8-9a80-4cdc-8978-6258326192d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Machine learning is a subset of artificial intelligence that involves training algorithms to learn from data and make predictions, decisions, or take actions without being explicitly programmed for a specific task.\n",
            "\n",
            "Time: 0.23 seconds\n",
            "That's FAST! ðŸš€\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from llm_playbook import GroqClient\n",
        "\n",
        "# Initialize the client (uses llama-3.3-70b-versatile by default)\n",
        "client = GroqClient()\n",
        "\n",
        "# Time the response\n",
        "start = time.time()\n",
        "response = client.chat(\"What is machine learning in one sentence?\")\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Response: {response}\")\n",
        "print(f\"\\nTime: {elapsed:.2f} seconds\")\n",
        "print(\"That's FAST! ðŸš€\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWNksHOAFl56"
      },
      "outputs": [],
      "source": [
        "# Compare with a longer response\n",
        "start = time.time()\n",
        "response = client.chat(\n",
        "    \"Explain the difference between supervised and unsupervised learning.\",\n",
        "    max_tokens=200\n",
        ")\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Response: {response}\")\n",
        "print(f\"\\nTime: {elapsed:.2f} seconds for ~200 tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IJhnaUgFl56"
      },
      "source": [
        "## 2. Available Models\n",
        "\n",
        "Groq hosts several open-source models. Let's try them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtNBTIflFl57"
      },
      "outputs": [],
      "source": [
        "# Llama 3.3 70B (default)\n",
        "llama_client = GroqClient(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "response = llama_client.chat(\"What makes you different from other language models?\")\n",
        "print(\"Llama 3.3 70B:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8fmBui1Fl57"
      },
      "outputs": [],
      "source": [
        "# Llama 3.1 8B - even faster!\n",
        "small_client = GroqClient(model=\"llama-3.1-8b-instant\")\n",
        "\n",
        "start = time.time()\n",
        "response = small_client.chat(\"What is 2+2? Just the number.\")\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Llama 3.1 8B: {response}\")\n",
        "print(f\"Time: {elapsed:.3f} seconds - instant!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4Uj9ImNFl58"
      },
      "outputs": [],
      "source": [
        "# Mixtral 8x7B\n",
        "mixtral_client = GroqClient(model=\"mixtral-8x7b-32768\")\n",
        "\n",
        "response = mixtral_client.chat(\"Write a haiku about speed.\")\n",
        "print(\"Mixtral 8x7B:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnuiYB0lFl58"
      },
      "outputs": [],
      "source": [
        "# Gemma 2 9B\n",
        "gemma_client = GroqClient(model=\"gemma2-9b-it\")\n",
        "\n",
        "response = gemma_client.chat(\"What's unique about Google's Gemma model?\")\n",
        "print(\"Gemma 2 9B:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k54QvWdWFl58"
      },
      "source": [
        "## 3. System Prompts\n",
        "\n",
        "Control the model's behavior with system prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VswiwAzpFl58"
      },
      "outputs": [],
      "source": [
        "# Code assistant persona\n",
        "response = client.chat(\n",
        "    message=\"How do I read a CSV file?\",\n",
        "    system_prompt=\"You are a Python expert. Give brief, practical code examples. No explanations, just code.\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm6w8gtRFl5-"
      },
      "outputs": [],
      "source": [
        "# JSON output mode\n",
        "response = client.chat(\n",
        "    message=\"List 3 colors\",\n",
        "    system_prompt=\"Respond only in valid JSON format. No markdown, no explanation.\",\n",
        "    temperature=0.0\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0CqdqqAFl5-"
      },
      "source": [
        "## 4. Multi-turn Conversations\n",
        "\n",
        "Maintain context across multiple exchanges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xan-iUihFl5-",
        "outputId": "9cfddc97-45b7-4378-efae-14f79f0b5a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student: What is a Python list?\n",
            "Tutor: **Python List**: A mutable, ordered collection of elements that can be of any data type, including strings, integers, floats, and other lists. Defined using square brackets `[]`. Example: `my_list = [1, 2, 3, \"hello\"]`\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from llm_playbook import ChatMessage\n",
        "\n",
        "history = []\n",
        "system = \"You are a helpful coding tutor. Be concise.\"\n",
        "\n",
        "# Turn 1\n",
        "q1 = \"What is a Python list?\"\n",
        "a1 = client.chat(q1, system_prompt=system, history=history)\n",
        "\n",
        "print(f\"Student: {q1}\")\n",
        "print(f\"Tutor: {a1}\\n\")\n",
        "\n",
        "history.append(ChatMessage(role=\"user\", content=q1))\n",
        "history.append(ChatMessage(role=\"assistant\", content=a1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEYAQgrpFl5-",
        "outputId": "c9273487-e617-462a-a4f5-b3c297e7fdf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student: How do I add items to it?\n",
            "Tutor: **Adding Items to a List**:\n",
            "\n",
            "* **Append**: Add a single item to the end of the list using `append()`. Example: `my_list.append(4)`\n",
            "* **Extend**: Add multiple items to the end of the list using `extend()`. Example: `my_list.extend([5, 6, 7])`\n",
            "* **Insert**: Add an item at a specific position using `insert()`. Example: `my_list.insert(0, 0)`\n"
          ]
        }
      ],
      "source": [
        "# Turn 2\n",
        "q2 = \"How do I add items to it?\"\n",
        "a2 = client.chat(q2, system_prompt=system, history=history)\n",
        "\n",
        "print(f\"Student: {q2}\")\n",
        "print(f\"Tutor: {a2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMjgkxtRFl5-"
      },
      "source": [
        "## 5. Streaming - Real-time Output\n",
        "\n",
        "Stream tokens as they're generated. Even with Groq's speed, streaming provides better UX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XhgpMQVFl5-",
        "outputId": "544234a6-cd78-4434-e377-ce964d4497d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming: There once was a coder so fine,\n",
            "Whose programs were truly divine.\n",
            "She coded with care,\n",
            "And debugged with flair,\n",
            "And her apps were always on time.\n"
          ]
        }
      ],
      "source": [
        "print(\"Streaming: \", end=\"\")\n",
        "\n",
        "for token in client.stream(\"Write a limerick about programming.\"):\n",
        "    print(token, end=\"\", flush=True)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpdVQ_KLFl5-"
      },
      "outputs": [],
      "source": [
        "# Streaming a longer response\n",
        "print(\"Streaming story:\\n\")\n",
        "\n",
        "for token in client.stream(\n",
        "    message=\"Write a 4-sentence story about a robot.\",\n",
        "    max_tokens=150\n",
        "):\n",
        "    print(token, end=\"\", flush=True)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqDM_t7BFl5-"
      },
      "source": [
        "## 6. Speed Comparison\n",
        "\n",
        "Let's benchmark different models on Groq."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2zVxIoSFl5-"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "    (\"llama-3.1-8b-instant\", \"Llama 3.1 8B\"),\n",
        "    (\"gemma2-9b-it\", \"Gemma 2 9B\"),\n",
        "    (\"mixtral-8x7b-32768\", \"Mixtral 8x7B\"),\n",
        "    (\"llama-3.3-70b-versatile\", \"Llama 3.3 70B\"),\n",
        "]\n",
        "\n",
        "prompt = \"What is Python? Answer in one sentence.\"\n",
        "print(f\"Prompt: {prompt}\\n\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for model_id, name in models:\n",
        "    try:\n",
        "        test_client = GroqClient(model=model_id)\n",
        "        start = time.time()\n",
        "        response = test_client.chat(prompt)\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"\\n{name} ({elapsed:.2f}s):\")\n",
        "        print(f\"  {response[:100]}...\" if len(response) > 100 else f\"  {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{name}: Error - {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1HHxMjWFl5-"
      },
      "source": [
        "## 7. When to Use Groq\n",
        "\n",
        "Groq is ideal for:\n",
        "\n",
        "- **Real-time applications**: Chatbots, live coding assistants\n",
        "- **High-throughput**: Processing many requests quickly\n",
        "- **Prototyping**: Fast iteration during development\n",
        "- **Cost-sensitive**: Free tier + open models\n",
        "\n",
        "Consider other providers when:\n",
        "- You need the absolute best quality (try Claude or GPT-4)\n",
        "- You need multimodal capabilities (images, audio)\n",
        "- You need very long context windows (1M+ tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zgKA-b5Fl5-"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You've learned:\n",
        "\n",
        "1. **Speed**: Groq is incredibly fast - often sub-second responses\n",
        "2. **Models**: Access to Llama, Mixtral, Gemma, and more\n",
        "3. **Usage**: Same familiar interface as other providers\n",
        "4. **Streaming**: Real-time output for better UX\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Try the [Ollama notebook](05_ollama.ipynb) for local LLM inference\n",
        "- Check out [06_comparison.ipynb](06_comparison.ipynb) for side-by-side comparisons\n",
        "- Explore Groq for building real-time AI applications"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}